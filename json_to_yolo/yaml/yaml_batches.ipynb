{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Move two directories up from the current file location\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))\n",
    "\n",
    "# Now import the LoggerManager\n",
    "from Research_docs.utils.my_logger_module import LoggerManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define updated category mapping based on new labels\n",
    "CATEGORY_MAPPING = {\n",
    "    \"person\": 0,\n",
    "    \"pedestrian\": 0,  # Merge pedestrian into person\n",
    "    \"rider\": 1,\n",
    "    \"car\": 2,\n",
    "    \"truck\": 3,\n",
    "    \"bus\": 4,\n",
    "    \"train\": 5,\n",
    "    \"motor\": 6,  # Motorcycle\n",
    "    \"motorcycle\": 6,  # Merge motorcycle into motor\n",
    "    \"bike\": 7,  # Bicycle\n",
    "    \"bicycle\": 7,  # Merge bicycle into bike\n",
    "    \"traffic light\": 8,\n",
    "    \"traffic sign\": 9,\n",
    "    \"trailer\": 10,\n",
    "    \"other person\": 11,\n",
    "    \"other vehicle\": 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Ensure the 'names' list follows correct index order\n",
    "CATEGORY_NAMES = [name for name, index in sorted(CATEGORY_MAPPING.items(), key=lambda item: item[1])]\n",
    "\n",
    "def ensure_data_yaml_exists(batch_root):\n",
    "    \"\"\"Ensures that a valid data.yaml file exists in each batch directory and always overwrites it.\"\"\"\n",
    "    logging.info(\"Creating/updating data.yaml for all batches...\")\n",
    "\n",
    "    if not os.path.exists(batch_root):\n",
    "        logging.error(f\"Batch root directory {batch_root} does not exist.\")\n",
    "        return\n",
    "\n",
    "    for batch in os.listdir(batch_root):\n",
    "        batch_path = os.path.join(batch_root, batch)\n",
    "        data_yaml_path = os.path.join(batch_path, 'data.yaml')\n",
    "\n",
    "        if os.path.isdir(batch_path):\n",
    "            try:\n",
    "                logging.info(f\"Overwriting data.yaml in {batch_path}...\")\n",
    "                data_yaml_content = {\n",
    "                    'path': os.path.abspath(batch_path),\n",
    "                    'train': os.path.join(batch_path, 'train.txt'),\n",
    "                    'val': os.path.join(batch_path, 'val.txt'),\n",
    "                    'test': os.path.join(batch_path, 'test.txt'),\n",
    "                    'names': CATEGORY_NAMES  # Now stored as a properly formatted list\n",
    "                }\n",
    "                with open(data_yaml_path, 'w', encoding='utf-8') as f:\n",
    "                    yaml.dump(data_yaml_content, f, default_flow_style=False, sort_keys=False)\n",
    "                logging.info(f\"data.yaml updated successfully in {batch_path}.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error updating data.yaml in {batch_path}: {e}\")\n",
    "\n",
    "def create_splits(batch_root):\n",
    "    \"\"\"Creates or updates train.txt, val.txt, and test.txt files listing image paths.\"\"\"\n",
    "    logging.info(\"Creating/updating dataset split files (train.txt, val.txt, test.txt) for all batches...\")\n",
    "\n",
    "    if not os.path.exists(batch_root):\n",
    "        logging.error(f\"Batch root directory {batch_root} does not exist.\")\n",
    "        return\n",
    "\n",
    "    for batch in os.listdir(batch_root):\n",
    "        batch_path = os.path.join(batch_root, batch)\n",
    "        if os.path.isdir(batch_path):\n",
    "            train_file = os.path.join(batch_path, \"train.txt\")\n",
    "            val_file = os.path.join(batch_path, \"val.txt\")\n",
    "            test_file = os.path.join(batch_path, \"test.txt\")\n",
    "\n",
    "            train_list, val_list, test_list = [], [], []\n",
    "\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                images_dir = os.path.join(batch_path, split, \"images\")\n",
    "                if os.path.exists(images_dir):\n",
    "                    image_files = [os.path.join(batch_path, split, \"images\", f) for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png'))]\n",
    "                    if split == \"train\":\n",
    "                        train_list.extend(image_files)\n",
    "                    elif split == \"val\":\n",
    "                        val_list.extend(image_files)\n",
    "                    elif split == \"test\":\n",
    "                        test_list.extend(image_files)\n",
    "\n",
    "            try:\n",
    "                with open(train_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write('\\n'.join(train_list) + '\\n')\n",
    "                logging.info(f\"train.txt updated in {batch_path}.\")\n",
    "                \n",
    "                with open(val_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write('\\n'.join(val_list) + '\\n')\n",
    "                logging.info(f\"val.txt updated in {batch_path}.\")\n",
    "                \n",
    "                with open(test_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write('\\n'.join(test_list) + '\\n')\n",
    "                logging.info(f\"test.txt updated in {batch_path}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error updating dataset split files for {batch}: {e}\")\n",
    "\n",
    "def save_directory_structure(batch_root):\n",
    "    \"\"\"Scans each batch and saves the directory structure to a file.\"\"\"\n",
    "    logging.info(\"Saving directory structure for each batch...\")\n",
    "\n",
    "    if not os.path.exists(batch_root):\n",
    "        logging.error(f\"Batch root directory {batch_root} does not exist.\")\n",
    "        return\n",
    "\n",
    "    for batch in os.listdir(batch_root):\n",
    "        batch_path = os.path.join(batch_root, batch)\n",
    "        structure_file = os.path.join(batch_path, \"directory_structure.txt\")\n",
    "\n",
    "        if os.path.isdir(batch_path):\n",
    "            try:\n",
    "                logging.info(f\"Writing directory structure in {batch_path}...\")\n",
    "                \n",
    "                # Define expected files and their mapped names\n",
    "                expected_files = {\n",
    "                    \"train\": \"train.txt\",\n",
    "                    \"val\": \"val.txt\",\n",
    "                    \"test\": \"test.txt\"\n",
    "                }\n",
    "\n",
    "                structure_content = []\n",
    "                for key, filename in expected_files.items():\n",
    "                    file_path = os.path.join(batch_path, filename)\n",
    "                    if os.path.exists(file_path):\n",
    "                        structure_content.append(f\"{key}: {filename}\")\n",
    "\n",
    "                # Write the structure to a file\n",
    "                with open(structure_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write('\\n'.join(structure_content) + '\\n')\n",
    "\n",
    "                logging.info(f\"directory_structure.txt saved in {batch_path}.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error writing directory structure in {batch_path}: {e}\")\n",
    "\n",
    "def process_batches(batch_root):\n",
    "    \"\"\"Main function to process dataset batches.\"\"\"\n",
    "    logging.info(\"Starting batch processing... Overwriting all files with latest changes.\")\n",
    "    ensure_data_yaml_exists(batch_root)\n",
    "    create_splits(batch_root)\n",
    "    save_directory_structure(batch_root)\n",
    "    logging.info(\"Batch processing completed successfully.\")\n",
    "    print(\"Batch processing completed. Check logs/process.log for details.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_root = r\"C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\my-project\\bdd100_mini\"  # Update this path if necessary\n",
    "    process_batches(batch_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def create_mini_dataset(source_root, dest_root, num_images=10, num_batches=3):\n",
    "    \"\"\"Creates a mini version of the dataset by randomly selecting images and copying their labels.\"\"\"\n",
    "    if not os.path.exists(source_root):\n",
    "        print(f\"Error: Source directory '{source_root}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(dest_root, exist_ok=True)\n",
    "\n",
    "    for batch_num in range(1, num_batches + 1):\n",
    "        source_batch = os.path.join(source_root, f\"batch_{batch_num}\")\n",
    "        dest_batch = os.path.join(dest_root, f\"batch_{batch_num}\")\n",
    "        \n",
    "        if not os.path.exists(source_batch):\n",
    "            print(f\"Skipping batch {batch_num}, source directory missing: {source_batch}\")\n",
    "            continue\n",
    "\n",
    "        os.makedirs(dest_batch, exist_ok=True)\n",
    "\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            source_split = os.path.join(source_batch, split, \"images\")\n",
    "            dest_split_images = os.path.join(dest_batch, split, \"images\")\n",
    "            dest_split_labels = os.path.join(dest_batch, split, \"labels\")\n",
    "\n",
    "            os.makedirs(dest_split_images, exist_ok=True)\n",
    "            os.makedirs(dest_split_labels, exist_ok=True)\n",
    "\n",
    "            if not os.path.exists(source_split):\n",
    "                print(f\"Skipping {split} in batch {batch_num}, source split missing: {source_split}\")\n",
    "                continue\n",
    "\n",
    "            # Get all image files\n",
    "            image_files = [f for f in os.listdir(source_split) if f.endswith(('.jpg', '.png'))]\n",
    "            if not image_files:\n",
    "                print(f\"No images found in {source_split}\")\n",
    "                continue\n",
    "\n",
    "            # Randomly select `num_images` images\n",
    "            selected_images = random.sample(image_files, min(num_images, len(image_files)))\n",
    "\n",
    "            for image in selected_images:\n",
    "                source_image_path = os.path.join(source_split, image)\n",
    "                dest_image_path = os.path.join(dest_split_images, image)\n",
    "\n",
    "                # Copy the image\n",
    "                shutil.copy2(source_image_path, dest_image_path)\n",
    "\n",
    "                # Find and copy the corresponding label\n",
    "                label_name = os.path.splitext(image)[0] + \".txt\"\n",
    "                source_label_path = os.path.join(source_batch, split, \"labels\", label_name)\n",
    "                dest_label_path = os.path.join(dest_split_labels, label_name)\n",
    "\n",
    "                if os.path.exists(source_label_path):\n",
    "                    shutil.copy2(source_label_path, dest_label_path)\n",
    "\n",
    "        # Copy data.yaml\n",
    "        source_yaml = os.path.join(source_batch, \"data.yaml\")\n",
    "        dest_yaml = os.path.join(dest_batch, \"data.yaml\")\n",
    "        if os.path.exists(source_yaml):\n",
    "            shutil.copy2(source_yaml, dest_yaml)\n",
    "\n",
    "    print(f\"Mini dataset created successfully at {dest_root}\")\n",
    "\n",
    "# Define source and destination paths\n",
    "source_root = r\"C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\my-project\\data\\bdd100_batch\"\n",
    "dest_root = r\"C:\\Users\\sathish\\Downloads\\FL_ModelForAV\\my-project\\data\\bdd100_mini\"\n",
    "\n",
    "# Run the function to create the mini dataset\n",
    "create_mini_dataset(source_root, dest_root, num_images=10, num_batches=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flower2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
